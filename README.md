# fakenews

 How can we approach fake news detection without relying on a fact-checking mechanism or unrealistic metadata? As we navigate the limited amount of data for this task, which augmentation methods are still effective after pre-trained, transformer-based models rendering many of them redundant? We address both of these issues as we approach the LIAR dataset (12,836 samples) with popular variants of BERT (DistillBERT, BERT, RoBERTa) and introduce a mix of random-noise augmentation methods in response to the overfitting observed during training. We achieve a state-of-the-art performance (30.48\%) on the LIAR dataset doing 1 point better than the best comparable models. We opt out of integrating the metadata provided by LIAR such as lying history as these are key information that is most likely unavailable in practice and can be avoided by adversarial agents in settings such as social media. We hold the hypothesis that the only way to detect unreliability in statements without fact-checking mechanisms are finding patterns in rhetorical structures such as phrasing and syntax. Our experiments with various language models point to this hypothesis although we surprisingly find that DistilBERT is the highest performing model and that our noise-based augmentation methods only prove effective for RoBERTa.
